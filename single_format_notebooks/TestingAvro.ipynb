{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01b6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import netCDF4 as nc\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import fastparquet as fpq\n",
    "import fastavro as avro\n",
    "import uuid\n",
    "import os\n",
    "from time import time_ns\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c00f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions and values\n",
    "def make_test_meta():\n",
    "    test_meta = {'uuid': str(uuid.uuid1()),\n",
    "                'param1':12,\n",
    "                'param2':'a_string',\n",
    "                'param3': np.random.rand()*1e9,\n",
    "                'param4':['alist','of','strings']}\n",
    "    return test_meta\n",
    "    \n",
    "\n",
    "def make_test_data(length_of_data):\n",
    "    test_data = {'time' :(np.array(range(length_of_data))*1e-9), \n",
    "                 'vals' : np.random.randn(length_of_data),\n",
    "                 'volts': np.random.randn(length_of_data), \n",
    "                 'dp'   : np.random.randn(length_of_data),\n",
    "                 'dr'   : np.random.randn(length_of_data)}\n",
    "    return test_data\n",
    "\n",
    "def get_col_names():\n",
    "    return ['time', 'vals', 'volts', 'dp', 'dr']\n",
    "def round_4(val):\n",
    "    return round(val, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "0fc08e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avro methods\n",
    "def Avro_make_meta_groups(n_to_write):\n",
    "    metadata_rows = {}\n",
    "    for i in range(n_to_write):\n",
    "        test_meta = json.dumps(make_test_meta())\n",
    "        metadata_rows[test_meta] = i\n",
    "    return metadata_rows\n",
    "\n",
    "def Avro_get_schema():\n",
    "    schema_1 = {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"Experiment details\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"meta_exper_name\",\n",
    "        \"type\" : \"string\"\n",
    "      }, {\n",
    "        \"name\" : \"data\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"record\",\n",
    "          \"name\" : \"data\",\n",
    "          \"fields\" : [ {\n",
    "            \"name\" : \"time\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"array\",\n",
    "              \"items\" : \"double\"\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"vals\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"array\",\n",
    "              \"items\" : \"double\"\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"volts\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"array\",\n",
    "              \"items\" : \"double\"\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"dr\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"array\",\n",
    "              \"items\" : \"double\"\n",
    "            }\n",
    "          }, {\n",
    "            \"name\" : \"dp\",\n",
    "            \"type\" : {\n",
    "              \"type\" : \"array\",\n",
    "              \"items\" : \"double\"\n",
    "            }\n",
    "          } ]\n",
    "        }\n",
    "      } ]\n",
    "    }\n",
    "    return avro.parse_schema(schema_1)\n",
    "    \n",
    "\n",
    "def Avro_make_record(metadata, length_of_data):\n",
    "    record ={'meta_exper_name' : metadata, 'data':make_test_data(length_of_data)}\n",
    "    return record  # records must be an iterable\n",
    "\n",
    "def Avro_write_n_to_file(length_of_data, n_to_write, fp = None):\n",
    "    \n",
    "    if fp is None:\n",
    "        fp = 'AvroTestFile1.avro'\n",
    "        \n",
    "    if os.path.exists(fp):\n",
    "        os.unlink(fp)\n",
    "    \n",
    "    then = time_ns()\n",
    "    my_schema = Avro_get_schema()\n",
    "    with open(fp, 'a+b') as file:\n",
    "        record_mappings = Avro_make_meta_groups(n_to_write)\n",
    "        custom_metadata = {'records_meta': json.dumps(record_mappings)}\n",
    "        for i, exp_meta in zip(range(n_to_write),record_mappings.keys()) : \n",
    "            record = [make_record(exp_meta, length_of_data)]\n",
    "            avro.writer(file, my_schema, record, metadata = custom_metadata) \n",
    "            # metadata is only written the first time\n",
    "\n",
    "    now = time_ns()\n",
    "    file_size = os.path.getsize(fp)/1000 #to get in kb\n",
    "    return (now-then) * 1e-9, file_size\n",
    "\n",
    "\n",
    "def Avro_load(source):\n",
    "    then = time_ns()\n",
    "    \n",
    "    with open(fp, 'rb') as fo:\n",
    "        avro_reader = avro.reader(fo)\n",
    "        record_mappings = json.loads(avro_reader.metadata['records_meta'])\n",
    "        metadata_read = (time_ns() - then)*1e-9\n",
    "\n",
    "        for record in avro_reader:\n",
    "            values = record['data']\n",
    "\n",
    "    columns_read = (time_ns() - then)*1e-9 \n",
    "\n",
    "    return metadata_read, columns_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "83a0c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to write: 0.0051 s, time to metaread 0.0034 s and colread is 0.0112 s, size is 80.933 KB\n"
     ]
    }
   ],
   "source": [
    "# small test to check functionality\n",
    "fp = 'AvroTestFile1.avro'\n",
    "writetime, filesize = Avro_write_n_to_file(2000, 1, fp)\n",
    "metadata_read, columns_read = Avro_load(fp)\n",
    "# print(writetime, filesize)\n",
    "\n",
    "print(f'time to write: {round_4(writetime)} s, time to metaread {round_4(metadata_read)} s and colread is {round_4(columns_read)} s, size is {round_4(filesize)} KB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d323fc",
   "metadata": {},
   "source": [
    "Links to documentation/resources\n",
    "\n",
    "Objects:\n",
    "- [Fastavro.Writer](https://fastavro.readthedocs.io/en/latest/writer.html)\n",
    "\n",
    "\n",
    "Resources:\n",
    "- [Make an avro schema](http://avro4s-ui.landoop.com/)\n",
    "- [Schema documentation](https://avro.apache.org/docs/1.11.1/specification/)\n",
    "- [Panda Avro](https://github.com/ynqa/pandavro)\n",
    "- [Reading a records into a dataframe](https://martinhynar.medium.com/quick-into-to-avro-in-python-and-how-to-make-it-pandas-dataframe-37bd0ae30ac6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "1dcb92ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total blocks:  2\n",
      "5\n",
      "5\n",
      "Total blocks:  6\n",
      "block  0 has no. records: 400\n",
      "block  1 has no. records: 1\n",
      "block  2 has no. records: 400\n",
      "block  3 has no. records: 1\n",
      "block  4 has no. records: 400\n",
      "block  5 has no. records: 1\n"
     ]
    }
   ],
   "source": [
    "# Testing functions\n",
    "\n",
    "'''\n",
    "Schema = schema_1, each row is a metadata and all experiment records (like JSON)\n",
    "Test to check read/write using schema 1\n",
    "'''\n",
    "def test_schema_1(fp):\n",
    "    fp = 'example1.avro'\n",
    "    parsed_schema = avro.parse_schema(schema_1)\n",
    "    # Writing \n",
    "    if os.path.exists(fp):\n",
    "            os.unlink(fp)\n",
    "    n_to_write = 2\n",
    "    length_of_data = 5\n",
    "    metadata = []\n",
    "    with open(fp, 'a+b') as out:\n",
    "        record_mappings = Avro_make_meta_groups(n_to_write)\n",
    "        custom_metadata = {'records_meta': json.dumps(record_mappings)}\n",
    "        for i, exp_meta in zip(range(n_to_write),record_mappings.keys()) : \n",
    "            records = [make_record(exp_meta, length_of_data)] #records must be an iterable\n",
    "            avro.writer(out, parsed_schema, records, metadata = custom_metadata) # metadata is only written the first time\n",
    "    \n",
    "    # Counting\n",
    "    with open(fp, 'rb') as fo:\n",
    "        avro_reader = avro.block_reader(fo)\n",
    "        z = sum(1 for _ in avro_reader)\n",
    "        print(\"Total blocks: \", z) # Should match n_to_write\n",
    "    \n",
    "    # Reading\n",
    "    with open(fp, 'rb') as fo:\n",
    "        avro_reader = avro.reader(fo)\n",
    "#         print(\"meta:\\n \", json.loads(avro_reader.metadata['records_meta']).keys())\n",
    "        for record in avro_reader:\n",
    "            print(len(record['data']['time'])) # should match length_of_data\n",
    "#             print(record['data']) # gives back all testdata\n",
    "    return\n",
    "\n",
    " \n",
    "\n",
    "'''\n",
    "Schema = schema_2, each row is a record \n",
    "Test to see if each experiment can be a block but it seems that block size has a limit, if the size of records in the block\n",
    "is too big it splits into more blocks. It seems after 400 records the experiments splits into different blocks\n",
    "'''\n",
    "def check_if_blocks_work(fp):\n",
    "    fp = 'example2.avro'\n",
    "    parsed_schema = avro.parse_schema(schema_2)\n",
    "    # Writing \n",
    "    if os.path.exists(fp):\n",
    "            os.unlink(fp)\n",
    "    n_to_write = 3\n",
    "    length_of_data = 401\n",
    "    with open(fp, 'a+b') as out:\n",
    "        for i in range(n_to_write): # Each i should be a new block\n",
    "            df = pd.DataFrame(make_test_data(length_of_data))\n",
    "            records = df.to_dict(orient='records')\n",
    "            avro.writer(out, parsed_schema, records)\n",
    "    # Counting\n",
    "    with open(fp, 'rb') as fo:\n",
    "        avro_reader = avro.block_reader(fo)\n",
    "        z = sum(1 for _ in avro_reader)\n",
    "        print(\"Total blocks: \", z) # Should match n_to_write\n",
    "    \n",
    "    # Records per block\n",
    "    with open(fp, 'rb') as fo:\n",
    "        avro_reader = avro.block_reader(fo)\n",
    "        for i, block in enumerate(avro_reader):\n",
    "            print(\"block \", i ,\"has no. records:\",block.num_records)\n",
    "    return\n",
    "\n",
    "# Run\n",
    "test_schema_1('example1.avro')\n",
    "check_if_blocks_work('example2.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e775097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schemas \n",
    "'''\n",
    "Records use the type name “record” \n",
    "name: a JSON string providing the name of the record (required).\n",
    "namespace, a JSON string that qualifies the name (optional);\n",
    "doc: a JSON string providing documentation to the user of this schema (optional).\n",
    "\n",
    "'''\n",
    "schema_1 = {\n",
    "  \"type\" : \"record\",\n",
    "  \"name\" : \"Experiment details\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"meta_exper_name\",\n",
    "    \"type\" : \"string\"\n",
    "  }, {\n",
    "    \"name\" : \"data\",\n",
    "    \"type\" : {\n",
    "      \"type\" : \"record\",\n",
    "      \"name\" : \"data\",\n",
    "      \"fields\" : [ {\n",
    "        \"name\" : \"time\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"array\",\n",
    "          \"items\" : \"double\"\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"vals\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"array\",\n",
    "          \"items\" : \"double\"\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"volts\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"array\",\n",
    "          \"items\" : \"double\"\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"dr\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"array\",\n",
    "          \"items\" : \"double\"\n",
    "        }\n",
    "      }, {\n",
    "        \"name\" : \"dp\",\n",
    "        \"type\" : {\n",
    "          \"type\" : \"array\",\n",
    "          \"items\" : \"double\"\n",
    "        }\n",
    "      } ]\n",
    "    }\n",
    "  } ]\n",
    "}\n",
    "\n",
    "schema_2 = {\n",
    "  \"type\" : \"record\",\n",
    "  \"name\" : \"Experiment details\",\n",
    "  \"fields\" : [ {\n",
    "    \"name\" : \"time\",\n",
    "    \"type\" : \"double\"\n",
    "  }, {\n",
    "    \"name\" : \"vals\",\n",
    "    \"type\" : \"double\"\n",
    "  }, {\n",
    "    \"name\" : \"volts\",\n",
    "    \"type\" : \"double\"\n",
    "  }, {\n",
    "    \"name\" : \"dr\",\n",
    "    \"type\" : \"double\"\n",
    "  }, {\n",
    "    \"name\" : \"dp\",\n",
    "    \"type\" : \"double\"\n",
    "  } ]\n",
    "}\n",
    "\n",
    "parsed_schema = avro.parse_schema(schema_2) #adds some extra details to for ease of writers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
