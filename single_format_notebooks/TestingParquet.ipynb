{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01b6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import netCDF4 as nc\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import fastparquet as fpq\n",
    "import uuid\n",
    "import os\n",
    "from time import time_ns\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3c00f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common functions and values\n",
    "def make_test_meta():\n",
    "    test_meta = {'uuid': str(uuid.uuid1()),\n",
    "                'param1':12,\n",
    "                'param2':'a_string',\n",
    "                'param3': np.random.rand()*1e9,\n",
    "                'param4':['alist','of','strings']}\n",
    "    return test_meta\n",
    "    \n",
    "\n",
    "def make_test_data(length_of_data):\n",
    "    test_data = {'time' :(np.array(range(length_of_data))*1e-9), \n",
    "                 'vals' : np.random.randn(length_of_data),\n",
    "                 'volts': np.random.randn(length_of_data), \n",
    "                 'dp'   : np.random.randn(length_of_data),\n",
    "                 'dr'   : np.random.randn(length_of_data)}\n",
    "    return test_data\n",
    "\n",
    "def get_col_names():\n",
    "    return ['time', 'vals', 'volts', 'dp', 'dr']\n",
    "def round_4(val):\n",
    "    return round(val, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fc08e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet methods\n",
    "def PQ_make_meta_groups(n_to_write):\n",
    "    metadata_rows = {}\n",
    "    for i in range(n_to_write):\n",
    "        test_meta = json.dumps(make_test_meta())\n",
    "        metadata_rows[test_meta] = i\n",
    "    return json.dumps(metadata_rows)\n",
    "\n",
    "\n",
    "def PQ_create_schema_w_meta(n_to_write):\n",
    "    df = pd.DataFrame(make_test_data(1)) # Small dataframe to get pandas schema\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    row_group_mappings = PQ_make_meta_groups(n_to_write)\n",
    "    custom_metadata = {'row_group_meta': row_group_mappings}\n",
    "    existing_metadata = table.schema.metadata\n",
    "    merged_metadata = { **custom_metadata, **existing_metadata }\n",
    "    table = table.replace_schema_metadata(merged_metadata)\n",
    "    return table.schema\n",
    "\n",
    "def PQ_write_n_to_file(length_of_data, n_to_write, fp = None):\n",
    "    \n",
    "    if fp is None:\n",
    "        fp = 'PQTestFile.parquet'\n",
    "        \n",
    "    if os.path.exists(fp):\n",
    "        os.unlink(fp)\n",
    "    \n",
    "    then = time_ns()\n",
    "    \n",
    "    my_schema = PQ_create_schema_w_meta(n_to_write)\n",
    "    with pq.ParquetWriter(fp, my_schema) as writer:\n",
    "        for i in range(n_to_write):\n",
    "            df = pd.DataFrame(make_test_data(length_of_data))\n",
    "            table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "            writer.write_table(table, row_group_size = length_of_data)\n",
    "\n",
    "    now = time_ns()\n",
    "    file_size = os.path.getsize(fp)/1000 #to get in kb\n",
    "    return (now-then) * 1e-9, file_size\n",
    "\n",
    "\n",
    "def PQ_load(source):\n",
    "    then = time_ns()\n",
    "\n",
    "    # accessing meta\n",
    "    metadata_pq = pq.read_metadata(source)\n",
    "    print(\"Row groups: \", metadata_pq.num_row_groups)\n",
    "    meta = metadata_pq.metadata[b'row_group_meta']\n",
    "    row_group_mappings = json.loads(meta)\n",
    "    metadata_read = (time_ns() - then)*1e-9\n",
    "\n",
    "\n",
    "    with pq.ParquetFile(source) as parquet_file:\n",
    "        for row_num in row_group_mappings.values():\n",
    "            table = parquet_file.read_row_group(row_num, columns=['time', 'vals','volts']).to_pandas()\n",
    "\n",
    "    columns_read = (time_ns() - then)*1e-9 \n",
    "\n",
    "    return metadata_read, columns_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83a0c7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row groups:  5\n",
      "time to write: 0.1738 s, time to metaread 0.0156 s and colread is 0.0313 s, size is 4911.615 KB\n"
     ]
    }
   ],
   "source": [
    "# small test to check functionality\n",
    "fp = 'PQTestFile.parquet'\n",
    "writetime, filesize = PQ_write_n_to_file(20000, 5, fp)\n",
    "metadata_read, columns_read = PQ_load(fp)\n",
    "# print(writetime, filesize)\n",
    "\n",
    "print(f'time to write: {round_4(writetime)} s, time to metaread {round_4(metadata_read)} s and colread is {round_4(columns_read)} s, size is {round_4(filesize)} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88cde3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastParquet methods\n",
    "def FPQ_make_meta_groups(n_to_write):\n",
    "    metadata_rows = {}\n",
    "    for i in range(n_to_write):\n",
    "        test_meta = json.dumps(make_test_meta())\n",
    "        metadata_rows[test_meta] = i\n",
    "    return json.dumps(metadata_rows)\n",
    "\n",
    "\n",
    "def FPQ_create_schema_w_meta(n_to_write):\n",
    "    df = pd.DataFrame(make_test_data(1)) # Small dataframe to get pandas schema\n",
    "    table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "    row_group_mappings = PQ_make_meta_groups(n_to_write)\n",
    "    custom_metadata = {'row_group_meta': row_group_mappings}\n",
    "    existing_metadata = table.schema.metadata\n",
    "    merged_metadata = { **custom_metadata, **existing_metadata }\n",
    "    table = table.replace_schema_metadata(merged_metadata)\n",
    "    return table.schema\n",
    "\n",
    "def FPQ_write_n_to_file(length_of_data, n_to_write, fp = None):\n",
    "    \n",
    "    if fp is None:\n",
    "        fp = 'FPQTestFile1.parq'\n",
    "        \n",
    "    if os.path.exists(fp):\n",
    "        os.unlink(fp)\n",
    "    \n",
    "    then = time_ns()\n",
    "    row_group_mappings = FPQ_make_meta_groups(n_to_write)\n",
    "    custom_metadata = {'row_group_meta': row_group_mappings}\n",
    "    all_data = []\n",
    "    \n",
    "    for i in range(n_to_write):\n",
    "        tes = make_test_data(length_of_data)\n",
    "        df = pd.DataFrame(tes)\n",
    "        all_data.append(df)\n",
    "#         df = pd.DataFrame(tes)\n",
    "#         table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "#         print(i, table)\n",
    "        \n",
    "#         append = False if i == 0 else True\n",
    "#         if i == 0:\n",
    "#         fpq.write(fp, df, row_group_offsets = length_of_data, append =append, custom_metadata = custom_metadata)\n",
    "#         else:\n",
    "#             file = fpq.ParquetFile(fp)\n",
    "#             file.write_row_groups(df, row_group_offsets = length_of_data)\n",
    "#         writer.write_table(table, row_group_size = length_of_data) , append =append, custom_metadata = custom_metadata \n",
    "    \n",
    "#     my_schema = PQ_create_schema_w_meta(n_to_write)\n",
    "#     with pq.ParquetWriter(fp, my_schema) as writer:\n",
    "#         for i in range(n_to_write):\n",
    "#             df = pd.DataFrame(make_test_data(length_of_data))\n",
    "#             table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "#             writer.write_table(table, row_group_size = length_of_data)\n",
    "\n",
    "    df = pd.concat(all_data)\n",
    "#     print(df)\n",
    "#     table = pa.Table.from_pandas(df, preserve_index=False)\n",
    "#     file = fpq.ParquetFile(fp)\n",
    "    fpq.write(fp, df, row_group_offsets = length_of_data, custom_metadata = custom_metadata)\n",
    "#     file.write_row_groups(df, row_group_offsets = length_of_data)\n",
    "#     print(table)\n",
    "\n",
    "    now = time_ns()\n",
    "    file_size = os.path.getsize(fp)/1000 #to get in kb \n",
    "    return (now-then) * 1e-9,  file_size\n",
    "\n",
    "\n",
    "def FPQ_load(source):\n",
    "    then = time_ns()\n",
    "\n",
    "    # accessing meta\n",
    "#     with fpq.ParquetFile('myfile.parq') as file:\n",
    "#     file = fpq.ParquetFile('myfile.parq')\n",
    "    file = fpq.ParquetFile(source)\n",
    "    print(\"Row groups: \", len(file.row_groups))\n",
    "    row_group_mappings = json.loads(file.key_value_metadata['row_group_meta'])\n",
    "\n",
    "    metadata_read = (time_ns() - then)*1e-9\n",
    "    \n",
    "    for rg in file.iter_row_groups(columns = ['time', 'vals']):\n",
    "        table = rg\n",
    "    # will generate every row group tho, you cant select just one\n",
    "\n",
    "    columns_read = (time_ns() - then)*1e-9 \n",
    "\n",
    "    return metadata_read, columns_read\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "106261b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row groups:  2\n",
      "time to write: 0.0235 s, time to metaread 0.0064 s and colread is 0.0156 s, size is 3.64 KB\n"
     ]
    }
   ],
   "source": [
    "fp = 'FPQTestFile1.parq'\n",
    "writetime, filesize = FPQ_write_n_to_file(10, 2, fp)\n",
    "metadata_read, columns_read = FPQ_load(fp)\n",
    "# print(writetime, filesize)\n",
    "print(f'time to write: {round_4(writetime)} s, time to metaread {round_4(metadata_read)} s and colread is {round_4(columns_read)} s, size is {round_4(filesize)} KB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "3c747525",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num\n",
      "0                time      vals\n",
      "index                        \n",
      "0      0.000000e+00 -2.155567\n",
      "1      1.000000e-09 -1.088836\n",
      "2      2.000000e-09  0.604460\n",
      "3      3.000000e-09 -2.034959\n",
      "4      4.000000e-09  0.367580\n",
      "5      5.000000e-09 -0.721668\n",
      "6      6.000000e-09 -1.632854\n",
      "7      7.000000e-09 -0.630668\n",
      "8      8.000000e-09 -0.309671\n",
      "9      9.000000e-09  0.683247\n",
      "num\n",
      "1                time      vals\n",
      "index                        \n",
      "0      0.000000e+00  0.996050\n",
      "1      1.000000e-09  0.152037\n",
      "2      2.000000e-09 -0.338334\n",
      "3      3.000000e-09 -2.119522\n",
      "4      4.000000e-09  1.521094\n",
      "5      5.000000e-09  0.099200\n",
      "6      6.000000e-09 -0.044801\n",
      "7      7.000000e-09  1.006903\n",
      "8      8.000000e-09  0.655576\n",
      "9      9.000000e-09 -0.011465\n"
     ]
    }
   ],
   "source": [
    "# with fpq.ParquetFile(fp) as file:\n",
    "   \n",
    "file = fpq.ParquetFile(fp)\n",
    "# print(file.statistics.keys()) # -> a dictionary of the stats keys\n",
    "# get a specific set of row groups\n",
    "# index_list = [0,2,4], renters insurance and pge\n",
    "# rows = [file.row_groups[i] for i in index_list]\n",
    "# for rg in rows:\n",
    "#     item = file.read_row_group_file(rg, columns = ['time', 'vals'], categories = None)\n",
    "#     print(item)\n",
    "# fpq.update_file_custom_metadata(fp, custom_metadata ={\"trial\" : \"value\"})\n",
    "# print(file.key_value_metadata)\n",
    "obj = json.loads(file.key_value_metadata['row_group_meta'])\n",
    "# print(obj)\n",
    "\n",
    "# filters=[('col3', 'in', [1, 2, 3, 4])])\n",
    "cols = ['time', 'vals']\n",
    "# new_file = file.to_pandas(columns = ['time', 'volts'] ,  filters=[('volts', 'in', [-2.23])])\n",
    "# print(new_file)\n",
    "for index, r in enumerate(file.iter_row_groups(columns = ['time', 'vals'])):\n",
    "    print(\"num\")\n",
    "    print(index, r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a77ce0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "0a32628f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyarrow._parquet.ColumnChunkMetaData object at 0x000001A6E13B0D10>\n",
      "  file_offset: 387519\n",
      "  file_path: \n",
      "  physical_type: DOUBLE\n",
      "  num_values: 20000\n",
      "  path_in_schema: vals\n",
      "  is_stats_set: True\n",
      "  statistics:\n",
      "    <pyarrow._parquet.Statistics object at 0x000001A6E13B0950>\n",
      "      has_min_max: True\n",
      "      min: -4.198500011324119\n",
      "      max: 3.862155340452514\n",
      "      null_count: 0\n",
      "      distinct_count: 0\n",
      "      num_values: 20000\n",
      "      physical_type: DOUBLE\n",
      "      logical_type: None\n",
      "      converted_type (legacy): NONE\n",
      "  compression: SNAPPY\n",
      "  encodings: ('RLE_DICTIONARY', 'PLAIN', 'RLE')\n",
      "  has_dictionary_page: True\n",
      "  dictionary_page_offset: 189865\n",
      "  data_page_offset: 349897\n",
      "  total_compressed_size: 197654\n",
      "  total_uncompressed_size: 197636\n"
     ]
    }
   ],
   "source": [
    "metadata_pq = pq.read_metadata(fp).row_group(0)\n",
    "print(metadata_pq.column(1))\n",
    "# metadata_pq = pq.read_metadata(fp) .column(1).statistics\n",
    "# print(metadata_pq.num_row_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d323fc",
   "metadata": {},
   "source": [
    "Links to documentation/resources\n",
    "\n",
    "Objects:\n",
    "- [Parquet.FileMetaData](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.FileMetaData.html#pyarrow.parquet.FileMetaData)\n",
    "- [Parquet.ParquetSchema](https://arrow.apache.org/docs/python/generated/pyarrow.parquet.ParquetSchema.html#pyarrow.parquet.ParquetSchema)\n",
    "- [Pyarrow.Table](https://arrow.apache.org/docs/python/generated/pyarrow.Table.html#pyarrow.Table)\n",
    "\n",
    "Resources:\n",
    "- [How to define a new schema](https://mungingdata.com/pyarrow/arbitrary-metadata-parquet-table/)\n",
    "- [Merge pandas schema + custom metadata](https://stackoverflow.com/questions/52122674/how-to-write-parquet-metadata-with-pyarrow)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "1dcb92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing functions\n",
    "\n",
    "'''\n",
    "pq.read_metadata returns an object pq.FileMetaData from footer of a single Parquet file. It contains info about num_columns,\n",
    "num_rows, num_row_groups. pq_meta.metadata stores additional user entered metadata, a dict that should\n",
    "have the metadata we created. It has 2 keys - custom_meta (the row group mapping), pandas (pandas metadata from dataframe)\n",
    "The keys/values are stored as byte strings\n",
    "pq_meta.schema will store the a pq.ParquetSchema object with the names of all the fields(columns)\n",
    "'''\n",
    "\n",
    "def print_metadata_(source):\n",
    "    pq_meta = pq.read_metadata(source)\n",
    "    print(\"FileMetaData: \", pq_meta)\n",
    "    print(\"Schema: \", pq_meta.schema)\n",
    "    \n",
    "    our_meta = pq_meta.metadata\n",
    "    print(\"Our meta keys: \",our_meta.keys())\n",
    "    \n",
    "    row_group_mappings = json.loads(our_meta[b'custom_meta'])\n",
    "    print(\"Row mapping: \", row_group_mappings)\n",
    "    \n",
    "    return\n",
    "\n",
    "'''\n",
    "\n",
    "metadata_pq.row_group(int) returns a pq.RowGroupMetaData object which stores num_columns/rows and size\n",
    "To access info about a specific column row_group_meta.column(<col no.>) this gives you data type, total size compr/uncompr\n",
    "and stats which has min/max values \n",
    "\n",
    "'''\n",
    "\n",
    "def print_row_group_meta(source):\n",
    "    metadata_pq = pq.read_metadata(source)\n",
    "    num_row_groups = metadata_pq.num_row_groups\n",
    "    \n",
    "    for i in range(num_row_groups):\n",
    "        row_group_meta = metadata_pq.row_group(i)\n",
    "        print(f\"All row {i} group meta \", row_group_meta)\n",
    "        \n",
    "        col_data = row_group_meta.column(1)\n",
    "        name = col_data.path_in_schema\n",
    "        print(f\"Column data for {name}: \", col_data)\n",
    "        \n",
    "        print(f\"Stats for {name} row group {i}\",col_data.statistics)\n",
    "    \n",
    "    return\n",
    "        \n",
    "    \n",
    "   \n",
    "'''\n",
    "pq.read_table returns a Pyarrow.Table object, an optimized data structure developed by Apache Arrow\n",
    ".the entire table with all row groups and columns. You can specify\n",
    "the columns you want to get a subset of cols and apply filters to get a subset of rows. It has attributes like\n",
    "column_names, num_rows/columns, schema. table.schema has the field names and types. table.schema.metadata is where the metadata \n",
    "we created is stored in a dict. You can also read the table as a pandas dataframe.\n",
    "\n",
    "'''\n",
    "def print_table(source):\n",
    "    \n",
    "    table = pq.read_table(source, columns=['time'])\n",
    "    print(\"pyarrow table: \",table )\n",
    "    print(\"pyarrow table: \",table.schema.metadata )\n",
    "    row_group_mappings = json.loads(table.schema.metadata[b'custom_meta'])\n",
    "    \n",
    "    \n",
    "    pandas_table = table.to_pandas()\n",
    "    print(\"pandas table: \", pandas_table)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
